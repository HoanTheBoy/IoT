import pandas as pd
import numpy as np
import joblib  # D√πng ƒë·ªÉ l∆∞u v√† t·∫£i m√¥ h√¨nh
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# 1. ƒê·ªçc d·ªØ li·ªáu
df = pd.read_csv("datafake.csv")

y = df["Price"]

# 3. X·ª≠ l√Ω d·ªØ li·ªáu - M√£ h√≥a c√°c c·ªôt d·∫°ng text
categorical_features = ["ScreenSize", "CPU", "GPU", "RAM", "Shell", "Battery", "Shop"]
encoder = OneHotEncoder(sparse_output=False, handle_unknown="ignore")
encoded_features = encoder.fit_transform(df[categorical_features])
encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out())

# 4. Gh√©p d·ªØ li·ªáu ƒë√£ m√£ h√≥a v·ªõi c√°c c·ªôt s·ªë
df_numeric = df.drop(columns=categorical_features + ["Name", "Price"])
df_final = pd.concat([df_numeric, encoded_df], axis=1)

# Ki·ªÉm tra l·∫°i s·ªë d√≤ng c·ªßa X v√† y
print(f"üõ† S·ªë d√≤ng c·ªßa X: {df_final.shape[0]}")
print(f"üõ† S·ªë d√≤ng c·ªßa y: {y.shape[0]}")

# 5. Chia t·∫≠p d·ªØ li·ªáu train/test
X = df_final

# Chu·∫©n h√≥a d·ªØ li·ªáu ƒë·∫ßu v√†o
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# 6. Hu·∫•n luy·ªán m√¥ h√¨nh
model = LinearRegression()
model.fit(X_train, y_train)

# 7. L∆∞u m√¥ h√¨nh, encoder, v√† scaler v√†o file
joblib.dump(model, "laptop_price_model.pkl")
joblib.dump(encoder, "encoder.pkl")
joblib.dump(scaler, "scaler.pkl")

print("‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng!")

# 8. ƒê√°nh gi√° m√¥ h√¨nh
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
print(f"üéØ R^2 Score: {r2:.4f}")
